{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH5qumQlg0g7"
      },
      "source": [
        "# Legal Document Analysis - Llama 3 Fine-tuning & DPO Training\n",
        "\n",
        "This notebook trains Llama 3 for legal document analysis with fine-tuning and DPO.\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KRbI4Ljg0g9",
        "outputId": "8db43308-5f12-4ede-be3f-609832fed058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected! Training will be very slow on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhfTBNbig0g-",
        "outputId": "cb1c727d-918e-4854-8e3f-7c7d8abf7e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m145.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch>=2.0.0 transformers>=4.35.0 accelerate>=0.24.0\n",
        "!pip install -q peft>=0.6.0 bitsandbytes>=0.41.0\n",
        "!pip install -q datasets>=2.14.0 trl>=0.7.0\n",
        "!pip install -q sentencepiece protobuf pandas numpy scikit-learn tqdm\n",
        "!pip install -q pyyaml python-dotenv pypdf2 python-docx nltk spacy\n",
        "!pip install -q evaluate\n",
        "\n",
        "# Download spaCy model\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lfZs5WTig0g_",
        "outputId": "187eb44e-9d9a-41bf-e97f-7324e080fc1b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1068082504.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive (optional - to save models and data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Set working directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (optional - to save models and data)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory\n",
        "import os\n",
        "WORK_DIR = '/content/legal-document-analysis'\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQhEmP4Xg0hA"
      },
      "source": [
        "## Upload Project Files\n",
        "\n",
        "You have two options:\n",
        "1. **Upload from GitHub** (recommended): Clone the repository\n",
        "2. **Upload manually**: Upload project files using the file browser on the left\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3VN_uWg0hA",
        "outputId": "00ce8d80-3c6d-4156-ebe7-9d02b8276a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'legal_opensource'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 61 (delta 18), reused 27 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 39.35 KiB | 13.12 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Clone from GitHub (if you have a repo)\n",
        "!git clone https://github.com/yourname/legal_opensource.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpxO1i11g0hB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg0l8qgAg0hB",
        "outputId": "1ed28685-5d7b-4e97-93f7-196bcb329ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variables set\n",
            "HF_TOKEN set: True\n",
            "\n",
            "⚠️ Make sure to set your actual HF_TOKEN above!\n"
          ]
        }
      ],
      "source": [
        "# Set your Hugging Face token\n",
        "import os\n",
        "\n",
        "# Option 1: Set directly (less secure, but convenient for Colab)\n",
        "os.environ['HF_TOKEN'] = 'hf'  # ⚠️ Replace with your token!\n",
        "\n",
        "# Option 2: Use Colab secrets (more secure - recommended)\n",
        "# from google.colab import userdata\n",
        "# os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Optional: Weights & Biases\n",
        "# os.environ['WANDB_API_KEY'] = 'your_wandb_key_here'\n",
        "\n",
        "print(\"Environment variables set\")\n",
        "print(f\"HF_TOKEN set: {bool(os.getenv('HF_TOKEN') and os.getenv('HF_TOKEN') != 'your_huggingface_token_here')}\")\n",
        "print(\"\\n⚠️ Make sure to set your actual HF_TOKEN above!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ictT8Yg0hB"
      },
      "source": [
        "## Generate All Source Code Files\n",
        "\n",
        "Creating all necessary Python files automatically...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae0z9diCg0hB",
        "outputId": "35c2898c-0565-4b40-e45a-e0ecbbffcd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ __init__.py files created\n"
          ]
        }
      ],
      "source": [
        "# Create __init__.py files\n",
        "with open('src/__init__.py', 'w') as f:\n",
        "    f.write('# Legal Document Analysis Tool\\n')\n",
        "\n",
        "with open('src/models/__init__.py', 'w') as f:\n",
        "    f.write('from .llama3_model import Llama3Model\\n\\n__all__ = [\\'Llama3Model\\']\\n')\n",
        "\n",
        "with open('src/data_processing/__init__.py', 'w') as f:\n",
        "    f.write('''from .legal_document_processor import LegalDocumentProcessor\n",
        "from .dataset_utils import (\n",
        "    load_jsonl, save_jsonl, create_finetune_dataset,\n",
        "    create_dpo_dataset, format_instruction_prompt\n",
        ")\n",
        "__all__ = ['LegalDocumentProcessor', 'load_jsonl', 'save_jsonl',\n",
        "           'create_finetune_dataset', 'create_dpo_dataset', 'format_instruction_prompt']\n",
        "''')\n",
        "\n",
        "with open('src/evaluation/__init__.py', 'w') as f:\n",
        "    f.write('from .citation_evaluator import CitationEvaluator\\n\\n__all__ = [\\'CitationEvaluator\\']\\n')\n",
        "\n",
        "print(\"✓ __init__.py files created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOd67E-8g0hC",
        "outputId": "13391e8d-70a1-4735-b580-74bb5b48b4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ llama3_model.py created\n"
          ]
        }
      ],
      "source": [
        "# Create llama3_model.py\n",
        "llama3_model_code = '''\"\"\"\n",
        "Llama 3 Model Integration and Loading\n",
        "\"\"\"\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import os\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "\n",
        "class Llama3Model:\n",
        "    \"\"\"Wrapper for Llama 3 model with quantization and LoRA support\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "        use_4bit: bool = True,\n",
        "        bnb_4bit_compute_dtype: str = \"float16\",\n",
        "        bnb_4bit_quant_type: str = \"nf4\",\n",
        "        bnb_4bit_use_double_quant: bool = True,\n",
        "        device_map: str = \"auto\",\n",
        "        trust_remote_code: bool = True\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.use_4bit = use_4bit\n",
        "        self.device_map = device_map\n",
        "\n",
        "        # Configure quantization\n",
        "        if use_4bit:\n",
        "            self.bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "                bnb_4bit_compute_dtype=getattr(torch, bnb_4bit_compute_dtype),\n",
        "                bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
        "            )\n",
        "        else:\n",
        "            self.bnb_config = None\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=trust_remote_code,\n",
        "            token=os.getenv(\"HF_TOKEN\")\n",
        "        )\n",
        "\n",
        "        # Set pad token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        # Load model\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the model with quantization if specified\"\"\"\n",
        "        print(f\"Loading model: {self.model_name}\")\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=self.bnb_config if self.use_4bit else None,\n",
        "            device_map=self.device_map,\n",
        "            trust_remote_code=True,\n",
        "            token=os.getenv(\"HF_TOKEN\"),\n",
        "            torch_dtype=torch.float16 if not self.use_4bit else None,\n",
        "        )\n",
        "\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "    def prepare_for_training(self, lora_config: LoraConfig):\n",
        "        \"\"\"Prepare model for LoRA fine-tuning\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not loaded. Call _load_model() first.\")\n",
        "\n",
        "        # Enable gradient checkpointing\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "\n",
        "        # Prepare model for k-bit training\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        # Apply LoRA\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        # Enable trainable parameters\n",
        "        self.model.print_trainable_parameters()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def get_model(self):\n",
        "        \"\"\"Get the underlying model\"\"\"\n",
        "        return self.model\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        \"\"\"Get the tokenizer\"\"\"\n",
        "        return self.tokenizer\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 512,\n",
        "        temperature: float = 0.7,\n",
        "        top_p: float = 0.9,\n",
        "        do_sample: bool = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Generate text from prompt\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not loaded\")\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                **kwargs\n",
        "            )\n",
        "\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "'''\n",
        "\n",
        "with open('src/models/llama3_model.py', 'w') as f:\n",
        "    f.write(llama3_model_code)\n",
        "\n",
        "print(\"✓ llama3_model.py created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBlWobXog0hC",
        "outputId": "cb813ee6-3d26-4aa6-e0d0-f7dd9a5a6d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ dataset_utils.py created\n"
          ]
        }
      ],
      "source": [
        "# Create dataset_utils.py (simplified version)\n",
        "dataset_utils_code = '''\"\"\"\n",
        "Dataset utilities for fine-tuning and DPO training\n",
        "\"\"\"\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from datasets import Dataset\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load data from JSONL file\"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def format_instruction_prompt(instruction: str, input_text: str = None) -> str:\n",
        "    \"\"\"Format instruction following Llama 3 chat template\"\"\"\n",
        "    if input_text:\n",
        "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful legal document analyst assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{instruction}\n",
        "\n",
        "Document:\n",
        "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful legal document analyst assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def preprocess_function_finetune(examples: Dict[str, List], tokenizer: PreTrainedTokenizer, max_length: int = 2048) -> Dict[str, Any]:\n",
        "    \"\"\"Preprocess function for fine-tuning\"\"\"\n",
        "    prompts = []\n",
        "    completions = []\n",
        "\n",
        "    for i in range(len(examples['prompt'])):\n",
        "        prompt = examples['prompt'][i]\n",
        "        completion = examples['completion'][i] if 'completion' in examples else examples.get('summary', [''])[i]\n",
        "        full_text = format_instruction_prompt(prompt, None) + completion\n",
        "        prompts.append(full_text)\n",
        "        completions.append(completion)\n",
        "\n",
        "    model_inputs = tokenizer(prompts, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    labels = model_inputs[\"input_ids\"].clone()\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
        "        prompt_len = len(prompt_tokens)\n",
        "        if prompt_len < max_length:\n",
        "            labels[i][:prompt_len] = -100\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def preprocess_function_dpo(examples: Dict[str, List], tokenizer: PreTrainedTokenizer, max_length: int = 2048, max_prompt_length: int = 512) -> Dict[str, Any]:\n",
        "    \"\"\"Preprocess function for DPO training\"\"\"\n",
        "    prompts = []\n",
        "    chosen = []\n",
        "    rejected = []\n",
        "\n",
        "    for i in range(len(examples['prompt'])):\n",
        "        prompt = examples['prompt'][i]\n",
        "        chosen_text = examples['chosen'][i]\n",
        "        rejected_text = examples['rejected'][i]\n",
        "        chosen_prompt = format_instruction_prompt(prompt, None) + chosen_text\n",
        "        rejected_prompt = format_instruction_prompt(prompt, None) + rejected_text\n",
        "        prompts.append(prompt)\n",
        "        chosen.append(chosen_prompt)\n",
        "        rejected.append(rejected_prompt)\n",
        "\n",
        "    tokenized_prompts = tokenizer(prompts, max_length=max_prompt_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    tokenized_chosen = tokenizer(chosen, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    tokenized_rejected = tokenizer(rejected, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": tokenized_prompts[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_prompts[\"attention_mask\"],\n",
        "        \"chosen_input_ids\": tokenized_chosen[\"input_ids\"],\n",
        "        \"chosen_attention_mask\": tokenized_chosen[\"attention_mask\"],\n",
        "        \"rejected_input_ids\": tokenized_rejected[\"input_ids\"],\n",
        "        \"rejected_attention_mask\": tokenized_rejected[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "\n",
        "def create_finetune_dataset(examples: List[Dict[str, Any]], tokenizer: PreTrainedTokenizer) -> Dataset:\n",
        "    \"\"\"Create fine-tuning dataset\"\"\"\n",
        "    dataset_dict = {\n",
        "        'prompt': [ex['prompt'] for ex in examples],\n",
        "        'completion': [ex.get('completion', ex.get('summary', '')) for ex in examples]\n",
        "    }\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    return dataset.map(lambda x: preprocess_function_finetune(x, tokenizer), batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "\n",
        "def create_dpo_dataset(examples: List[Dict[str, Any]], tokenizer: PreTrainedTokenizer) -> Dataset:\n",
        "    \"\"\"Create DPO dataset\"\"\"\n",
        "    dataset_dict = {\n",
        "        'prompt': [ex['prompt'] for ex in examples],\n",
        "        'chosen': [ex['chosen'] for ex in examples],\n",
        "        'rejected': [ex['rejected'] for ex in examples]\n",
        "    }\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    return dataset.map(lambda x: preprocess_function_dpo(x, tokenizer), batched=True, remove_columns=dataset.column_names)\n",
        "'''\n",
        "\n",
        "with open('src/data_processing/dataset_utils.py', 'w') as f:\n",
        "    f.write(dataset_utils_code)\n",
        "\n",
        "print(\"✓ dataset_utils.py created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVcZP724g0hC",
        "outputId": "05961660-2870-426e-b199-6cdec3dc34fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ citation_evaluator.py created\n"
          ]
        }
      ],
      "source": [
        "# Create citation_evaluator.py (simplified)\n",
        "citation_evaluator_code = '''\"\"\"\n",
        "Evaluation metrics for citation accuracy\n",
        "\"\"\"\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "class CitationEvaluator:\n",
        "    \"\"\"Evaluate citation accuracy\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.citation_patterns = [\n",
        "            r'\\\\d+\\\\s+[A-Z][a-z]+\\\\s+\\\\d+',\n",
        "            r'[A-Z][a-z]+\\\\s+v\\\\.\\\\s+[A-Z][a-z]+',\n",
        "            r'\\\\d+\\\\s+F\\\\.\\\\d+d\\\\s+\\\\d+',\n",
        "            r'\\\\d+\\\\s+F\\\\.\\\\s+Supp\\\\.\\\\s+\\\\d+',\n",
        "        ]\n",
        "\n",
        "    def extract_citations(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract all citations from text\"\"\"\n",
        "        citations = []\n",
        "        for pattern in self.citation_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            citations.extend(matches)\n",
        "        return list(set(citations))\n",
        "\n",
        "    def citation_precision(self, generated_text: str, source_document: str) -> float:\n",
        "        \"\"\"Calculate precision\"\"\"\n",
        "        generated_citations = self.extract_citations(generated_text)\n",
        "        source_citations = self.extract_citations(source_document)\n",
        "        if len(generated_citations) == 0:\n",
        "            return 1.0 if len(source_citations) == 0 else 0.0\n",
        "        correct_citations = sum(1 for cit in generated_citations if cit in source_citations)\n",
        "        return correct_citations / len(generated_citations)\n",
        "\n",
        "    def evaluate_summary(self, generated_summary: str, source_document: str, ground_truth_citations: List[str] = None) -> Dict[str, float]:\n",
        "        \"\"\"Comprehensive evaluation\"\"\"\n",
        "        return {\n",
        "            'citation_precision': self.citation_precision(generated_summary, source_document),\n",
        "            'num_citations_generated': len(self.extract_citations(generated_summary)),\n",
        "        }\n",
        "'''\n",
        "\n",
        "with open('src/evaluation/citation_evaluator.py', 'w') as f:\n",
        "    f.write(citation_evaluator_code)\n",
        "\n",
        "print(\"✓ citation_evaluator.py created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCr6qeW-g0hC",
        "outputId": "e947cf2d-1a11-46af-a614-e9caa0f13699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ train_finetune.py created\n"
          ]
        }
      ],
      "source": [
        "# Create train_finetune.py\n",
        "train_finetune_code = '''\"\"\"\n",
        "Fine-tuning script for Llama 3\n",
        "\"\"\"\n",
        "import os\n",
        "import yaml\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig\n",
        "from src.models.llama3_model import Llama3Model\n",
        "from src.data_processing.dataset_utils import load_jsonl, create_finetune_dataset\n",
        "\n",
        "\n",
        "def load_config(config_path: str) -> dict:\n",
        "    with open(config_path, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", type=str, default=\"configs/finetune_config.yaml\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config = load_config(args.config)\n",
        "    model_config = config['model']\n",
        "    lora_config_dict = config['lora']\n",
        "    training_config = config['training']\n",
        "    data_config = config['data']\n",
        "\n",
        "    print(\"Loading Llama 3 model...\")\n",
        "    llama_model = Llama3Model(\n",
        "        model_name=model_config['name'],\n",
        "        use_4bit=model_config['use_4bit'],\n",
        "        bnb_4bit_compute_dtype=model_config['bnb_4bit_compute_dtype'],\n",
        "        bnb_4bit_quant_type=model_config['bnb_4bit_quant_type'],\n",
        "        bnb_4bit_use_double_quant=model_config['bnb_4bit_use_double_quant']\n",
        "    )\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_config_dict['r'],\n",
        "        lora_alpha=lora_config_dict['lora_alpha'],\n",
        "        target_modules=lora_config_dict['target_modules'],\n",
        "        lora_dropout=lora_config_dict['lora_dropout'],\n",
        "        bias=lora_config_dict['bias'],\n",
        "        task_type=lora_config_dict['task_type']\n",
        "    )\n",
        "\n",
        "    model = llama_model.prepare_for_training(lora_config)\n",
        "    tokenizer = llama_model.get_tokenizer()\n",
        "\n",
        "    print(f\"Loading training data from {data_config['train_path']}...\")\n",
        "    train_examples = load_jsonl(data_config['train_path'])\n",
        "    if data_config.get('max_samples'):\n",
        "        train_examples = train_examples[:data_config['max_samples']]\n",
        "\n",
        "    val_examples = []\n",
        "    if data_config.get('val_path') and Path(data_config['val_path']).exists():\n",
        "        val_examples = load_jsonl(data_config['val_path'])\n",
        "\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = create_finetune_dataset(train_examples, tokenizer)\n",
        "    eval_dataset = create_finetune_dataset(val_examples, tokenizer) if val_examples else None\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=training_config['output_dir'],\n",
        "        num_train_epochs=training_config['num_train_epochs'],\n",
        "        per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=training_config['gradient_accumulation_steps'],\n",
        "        learning_rate=training_config['learning_rate'],\n",
        "        lr_scheduler_type=training_config['lr_scheduler_type'],\n",
        "        warmup_steps=training_config['warmup_steps'],\n",
        "        logging_steps=training_config['logging_steps'],\n",
        "        save_steps=training_config['save_steps'],\n",
        "        eval_steps=training_config['eval_steps'] if eval_dataset else None,\n",
        "        save_total_limit=training_config['save_total_limit'],\n",
        "        fp16=training_config['fp16'],\n",
        "        gradient_checkpointing=training_config['gradient_checkpointing'],\n",
        "        optim=training_config['optim'],\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(training_config['output_dir'])\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open('train_finetune.py', 'w') as f:\n",
        "    f.write(train_finetune_code)\n",
        "\n",
        "print(\"✓ train_finetune.py created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brwPxYXFg0hD",
        "outputId": "3904c5e2-8ecc-4385-c3b5-701f17f601a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ train_dpo.py created\n",
            "\n",
            "✅ All source files created!\n"
          ]
        }
      ],
      "source": [
        "# Create train_dpo.py\n",
        "train_dpo_code = '''\"\"\"\n",
        "DPO training script\n",
        "\"\"\"\n",
        "import os\n",
        "import yaml\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from transformers import TrainingArguments, AutoModelForCausalLM\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from src.models.llama3_model import Llama3Model\n",
        "from src.data_processing.dataset_utils import load_jsonl, create_dpo_dataset\n",
        "\n",
        "\n",
        "def load_config(config_path: str) -> dict:\n",
        "    with open(config_path, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", type=str, default=\"configs/dpo_config.yaml\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config = load_config(args.config)\n",
        "    model_config = config['model']\n",
        "    training_config = config['training']\n",
        "    dpo_config = config['dpo']\n",
        "    data_config = config['data']\n",
        "\n",
        "    print(f\"Loading base model from {model_config['base_model_path']}...\")\n",
        "    base_model_path = model_config['base_model_path']\n",
        "\n",
        "    llama_model = Llama3Model(\n",
        "        model_name=base_model_path if Path(base_model_path).exists() else model_config.get('name', \"meta-llama/Meta-Llama-3-8B-Instruct\"),\n",
        "        use_4bit=model_config['use_4bit'],\n",
        "        bnb_4bit_compute_dtype=model_config['bnb_4bit_compute_dtype'],\n",
        "        bnb_4bit_quant_type=model_config['bnb_4bit_quant_type'],\n",
        "        bnb_4bit_use_double_quant=model_config['bnb_4bit_use_double_quant']\n",
        "    )\n",
        "\n",
        "    if Path(base_model_path).exists():\n",
        "        try:\n",
        "            model = PeftModel.from_pretrained(llama_model.get_model(), base_model_path, device_map=\"auto\")\n",
        "            print(\"Loaded fine-tuned PEFT weights\")\n",
        "        except:\n",
        "            model = llama_model.get_model()\n",
        "            print(\"Using base model\")\n",
        "    else:\n",
        "        model = llama_model.get_model()\n",
        "\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_config.get('name', \"meta-llama/Meta-Llama-3-8B-Instruct\"),\n",
        "        quantization_config=llama_model.bnb_config if model_config['use_4bit'] else None,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        token=os.getenv(\"HF_TOKEN\"),\n",
        "        torch_dtype=torch.float16 if not model_config['use_4bit'] else None,\n",
        "    )\n",
        "\n",
        "    if Path(base_model_path).exists():\n",
        "        try:\n",
        "            ref_model = PeftModel.from_pretrained(ref_model, base_model_path, device_map=\"auto\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    tokenizer = llama_model.get_tokenizer()\n",
        "\n",
        "    print(f\"Loading DPO training data...\")\n",
        "    train_examples = load_jsonl(data_config['train_path'])\n",
        "    val_examples = load_jsonl(data_config['val_path']) if data_config.get('val_path') and Path(data_config['val_path']).exists() else []\n",
        "\n",
        "    train_dataset = create_dpo_dataset(train_examples, tokenizer)\n",
        "    eval_dataset = create_dpo_dataset(val_examples, tokenizer) if val_examples else None\n",
        "\n",
        "    dpo_training_args = DPOConfig(\n",
        "        output_dir=training_config['output_dir'],\n",
        "        num_train_epochs=training_config['num_train_epochs'],\n",
        "        per_device_train_batch_size=training_config['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=training_config['per_device_eval_batch_size'],\n",
        "        gradient_accumulation_steps=training_config['gradient_accumulation_steps'],\n",
        "        learning_rate=training_config['learning_rate'],\n",
        "        lr_scheduler_type=training_config['lr_scheduler_type'],\n",
        "        warmup_steps=training_config['warmup_steps'],\n",
        "        logging_steps=training_config['logging_steps'],\n",
        "        save_steps=training_config['save_steps'],\n",
        "        eval_steps=training_config['eval_steps'] if eval_dataset else None,\n",
        "        save_total_limit=training_config['save_total_limit'],\n",
        "        fp16=training_config['fp16'],\n",
        "        gradient_checkpointing=training_config['gradient_checkpointing'],\n",
        "        optim=training_config['optim'],\n",
        "        max_length=training_config['max_length'],\n",
        "        max_prompt_length=training_config['max_prompt_length'],\n",
        "        beta=dpo_config['beta'],\n",
        "        loss_type=dpo_config['loss_type'],\n",
        "    )\n",
        "\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model=model,\n",
        "        ref_model=ref_model,\n",
        "        args=dpo_training_args,\n",
        "        beta=dpo_config['beta'],\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=training_config['max_length'],\n",
        "        max_prompt_length=training_config['max_prompt_length'],\n",
        "    )\n",
        "\n",
        "    print(\"Starting DPO training...\")\n",
        "    dpo_trainer.train()\n",
        "    dpo_trainer.save_model()\n",
        "    tokenizer.save_pretrained(training_config['output_dir'])\n",
        "    print(\"DPO training completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "with open('train_dpo.py', 'w') as f:\n",
        "    f.write(train_dpo_code)\n",
        "\n",
        "print(\"✓ train_dpo.py created\")\n",
        "print(\"\\n✅ All source files created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWqacggmg0hD"
      },
      "source": [
        "## Create Configuration Files\n",
        "\n",
        "These configs are optimized for Colab's T4 GPU (16GB VRAM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGLZRw9Rg0hD",
        "outputId": "e5ae0070-01d7-4e63-a454-126c926264ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fine-tuning config created\n"
          ]
        }
      ],
      "source": [
        "# Create fine-tuning config (optimized for Colab)\n",
        "import yaml\n",
        "\n",
        "finetune_config = {\n",
        "    'model': {\n",
        "        'name': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
        "        'use_4bit': True,\n",
        "        'bnb_4bit_compute_dtype': 'float16',\n",
        "        'bnb_4bit_quant_type': 'nf4',\n",
        "        'bnb_4bit_use_double_quant': True\n",
        "    },\n",
        "    'lora': {\n",
        "        'r': 16,\n",
        "        'lora_alpha': 32,\n",
        "        'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "        'lora_dropout': 0.1,\n",
        "        'bias': 'none',\n",
        "        'task_type': 'CAUSAL_LM'\n",
        "    },\n",
        "    'training': {\n",
        "        'output_dir': './models/finetuned_llama3',\n",
        "        'num_train_epochs': 3,\n",
        "        'per_device_train_batch_size': 2,  # Reduced for Colab T4\n",
        "        'per_device_eval_batch_size': 2,\n",
        "        'gradient_accumulation_steps': 8,  # Increased to compensate\n",
        "        'learning_rate': 2e-4,\n",
        "        'lr_scheduler_type': 'cosine',\n",
        "        'warmup_steps': 100,\n",
        "        'logging_steps': 10,\n",
        "        'save_steps': 500,\n",
        "        'eval_steps': 500,\n",
        "        'save_total_limit': 3,\n",
        "        'fp16': True,\n",
        "        'gradient_checkpointing': True,\n",
        "        'optim': 'paged_adamw_32bit',\n",
        "        'max_seq_length': 2048\n",
        "    },\n",
        "    'data': {\n",
        "        'train_path': 'data/train.jsonl',\n",
        "        'val_path': 'data/val.jsonl',\n",
        "        'max_samples': None\n",
        "    }\n",
        "}\n",
        "\n",
        "os.makedirs('configs', exist_ok=True)\n",
        "with open('configs/finetune_config.yaml', 'w') as f:\n",
        "    yaml.dump(finetune_config, f)\n",
        "\n",
        "print(\"✓ Fine-tuning config created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTDyGDECg0hD",
        "outputId": "ce500b1f-4cc5-47d2-b984-3c267317f55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created 28 DPO training pairs\n",
            "✓ Created 2 DPO validation pairs\n",
            "\n",
            "✅ All training data generated!\n"
          ]
        }
      ],
      "source": [
        "# Generate DPO training data (chosen vs rejected pairs)\n",
        "dpo_examples = []\n",
        "\n",
        "for ex in finetune_examples:\n",
        "    prompt = ex['prompt']\n",
        "    chosen = ex['completion']  # Good response with citations\n",
        "\n",
        "    # Create rejected variants (without citations or with hallucinations)\n",
        "    rejected_variants = [\n",
        "        # Variant 1: Remove citations\n",
        "        re.sub(r'\\([^)]*\\d+\\s+[A-Z][a-z]+\\s+\\d+[^)]*\\)', '', chosen).strip(),\n",
        "        # Variant 2: Generic response without specific details\n",
        "        'The court made a decision based on the legal principles and facts presented in the case.',\n",
        "        # Variant 3: Add fake citation\n",
        "        chosen.replace('Smith v. Jones', 'Brown v. White, 789 F.2d 123 (2019)') if 'Smith' in chosen else chosen + ' (See Johnson v. Smith, 456 F.3d 789 (2021))'\n",
        "    ]\n",
        "\n",
        "    # Use first variant that's different from chosen\n",
        "    rejected = rejected_variants[0]\n",
        "    if rejected == chosen or len(rejected) < 20:\n",
        "        rejected = rejected_variants[1]\n",
        "\n",
        "    dpo_examples.append({\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen,\n",
        "        'rejected': rejected\n",
        "    })\n",
        "\n",
        "# Save DPO data\n",
        "with open('data/dpo_train.jsonl', 'w') as f:\n",
        "    for item in dpo_examples:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "with open('data/dpo_val.jsonl', 'w') as f:\n",
        "    for item in dpo_examples[:2]:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"✓ Created {len(dpo_examples)} DPO training pairs\")\n",
        "print(f\"✓ Created {len(dpo_examples[:2])} DPO validation pairs\")\n",
        "print(\"\\n✅ All training data generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjbMDiDEg0hD",
        "outputId": "0e9745e2-6e90-4bd4-dace-b78a127a5367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ DPO config created\n"
          ]
        }
      ],
      "source": [
        "# Create DPO config\n",
        "dpo_config = {\n",
        "    'model': {\n",
        "        'base_model_path': './models/finetuned_llama3',\n",
        "        'use_4bit': True,\n",
        "        'bnb_4bit_compute_dtype': 'float16',\n",
        "        'bnb_4bit_quant_type': 'nf4',\n",
        "        'bnb_4bit_use_double_quant': True\n",
        "    },\n",
        "    'training': {\n",
        "        'output_dir': './models/dpo_llama3',\n",
        "        'num_train_epochs': 2,\n",
        "        'per_device_train_batch_size': 1,  # Reduced for Colab\n",
        "        'per_device_eval_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 8,\n",
        "        'learning_rate': 1e-5,\n",
        "        'lr_scheduler_type': 'cosine',\n",
        "        'warmup_steps': 100,\n",
        "        'logging_steps': 10,\n",
        "        'save_steps': 500,\n",
        "        'eval_steps': 500,\n",
        "        'save_total_limit': 3,\n",
        "        'fp16': True,\n",
        "        'gradient_checkpointing': True,\n",
        "        'optim': 'paged_adamw_32bit',\n",
        "        'max_seq_length': 2048,\n",
        "        'max_prompt_length': 512\n",
        "    },\n",
        "    'dpo': {\n",
        "        'beta': 0.1,\n",
        "        'loss_type': 'sigmoid',\n",
        "        'label_smoothing': 0.0,\n",
        "        'reference_free': False\n",
        "    },\n",
        "    'data': {\n",
        "        'train_path': 'data/dpo_train.jsonl',\n",
        "        'val_path': 'data/dpo_val.jsonl',\n",
        "        'max_samples': None\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('configs/dpo_config.yaml', 'w') as f:\n",
        "    yaml.dump(dpo_config, f)\n",
        "\n",
        "print(\"✓ DPO config created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpDLmYY0g0hE"
      },
      "source": [
        "## Prepare Training Data\n",
        "\n",
        "Upload your training data files to the `data/` directory, or create sample data for testing below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CqHYIqEg0hE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbb0b13-1e16-44da-8652-42bcf4b6d0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created 28 fine-tuning examples\n",
            "✓ Created 5 validation examples\n",
            "✓ Training examples cover: Civil Procedure, Evidence, Constitutional Law, Employment, IP, Securities, Antitrust, Bankruptcy, and more\n"
          ]
        }
      ],
      "source": [
        "# Generate comprehensive training data (25+ examples covering diverse legal areas)\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Extensive fine-tuning data covering multiple legal domains\n",
        "finetune_examples = [\n",
        "    # Civil Procedure - Motion to Dismiss\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe court held that the defendant\\'s motion to dismiss should be granted. See Smith v. Jones, 123 F.3d 456 (2020). The plaintiff failed to state a claim upon which relief can be granted under Rule 12(b)(6) of the Federal Rules of Civil Procedure.\\n\\nCitations found: Smith v. Jones, 123 F.3d 456, Rule 12(b)(6)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The court granted the defendant\\'s motion to dismiss (Smith v. Jones, 123 F.3d 456 (2020)) because the plaintiff failed to state a claim under Federal Rule of Civil Procedure 12(b)(6).'\n",
        "    },\n",
        "    # Evidence - Hearsay\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nUnder the Federal Rules of Evidence, hearsay is generally inadmissible unless it falls within an exception. See Fed. R. Evid. 802. However, statements made by a party opponent are admissible as non-hearsay. See Fed. R. Evid. 801(d)(2).\\n\\nCitations found: Fed. R. Evid. 802, Fed. R. Evid. 801(d)(2)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Hearsay is generally inadmissible under Federal Rule of Evidence 802, but statements by party opponents are admissible as non-hearsay under Federal Rule of Evidence 801(d)(2).'\n",
        "    },\n",
        "    # Contracts - Statute of Limitations\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe statute of limitations for breach of contract is four years. See 28 U.S.C. § 1658. The plaintiff filed this action more than four years after the alleged breach occurred. Therefore, the claim is time-barred.\\n\\nCitations found: 28 U.S.C. § 1658\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The plaintiff\\'s breach of contract claim is time-barred under 28 U.S.C. § 1658, which establishes a four-year statute of limitations, because the action was filed more than four years after the alleged breach.'\n",
        "    },\n",
        "    # Civil Procedure - Summary Judgment\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe court applied the summary judgment standard from Celotex Corp. v. Catrett, 477 U.S. 317 (1986). Under this standard, the moving party must show there is no genuine issue of material fact. See Fed. R. Civ. P. 56(a).\\n\\nCitations found: Celotex Corp. v. Catrett, 477 U.S. 317, Fed. R. Civ. P. 56(a)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The court applied the summary judgment standard from Celotex Corp. v. Catrett, 477 U.S. 317 (1986), which requires the moving party to show no genuine issue of material fact exists under Federal Rule of Civil Procedure 56(a).'\n",
        "    },\n",
        "    # Criminal Procedure - Exclusionary Rule\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe exclusionary rule prohibits the use of evidence obtained in violation of the Fourth Amendment. See Mapp v. Ohio, 367 U.S. 643 (1961). However, the good faith exception applies when officers act in reasonable reliance on a warrant. See United States v. Leon, 468 U.S. 897 (1984).\\n\\nCitations found: Mapp v. Ohio, 367 U.S. 643, United States v. Leon, 468 U.S. 897\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The exclusionary rule from Mapp v. Ohio, 367 U.S. 643 (1961) prohibits use of evidence obtained in violation of the Fourth Amendment, but the good faith exception from United States v. Leon, 468 U.S. 897 (1984) applies when officers act in reasonable reliance on a warrant.'\n",
        "    },\n",
        "    # Civil Procedure - Personal Jurisdiction\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nFor a court to exercise personal jurisdiction over a non-resident defendant, the defendant must have minimum contacts with the forum state. See International Shoe Co. v. Washington, 326 U.S. 310 (1945). The contacts must be such that maintenance of the suit does not offend traditional notions of fair play and substantial justice. See Burger King Corp. v. Rudzewicz, 471 U.S. 462 (1985).\\n\\nCitations found: International Shoe Co. v. Washington, 326 U.S. 310, Burger King Corp. v. Rudzewicz, 471 U.S. 462\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Personal jurisdiction over a non-resident defendant requires minimum contacts with the forum state under International Shoe Co. v. Washington, 326 U.S. 310 (1945), and the contacts must satisfy traditional notions of fair play and substantial justice under Burger King Corp. v. Rudzewicz, 471 U.S. 462 (1985).'\n",
        "    },\n",
        "    # Civil Procedure - Class Actions\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nTo certify a class action, the plaintiff must satisfy the requirements of Federal Rule of Civil Procedure 23(a) and at least one subsection of Rule 23(b). See Fed. R. Civ. P. 23. The class must be so numerous that joinder is impracticable. See Fed. R. Civ. P. 23(a)(1).\\n\\nCitations found: Fed. R. Civ. P. 23, Fed. R. Civ. P. 23(a)(1)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Class action certification requires satisfying Federal Rule of Civil Procedure 23(a) and at least one subsection of Rule 23(b), including the numerosity requirement under Rule 23(a)(1) that the class be so numerous that joinder is impracticable.'\n",
        "    },\n",
        "    # Criminal Procedure - Miranda Rights\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nBefore custodial interrogation, law enforcement must inform suspects of their Miranda rights. See Miranda v. Arizona, 384 U.S. 436 (1966). These rights include the right to remain silent and the right to an attorney. See Miranda v. Arizona, 384 U.S. 436 (1966).\\n\\nCitations found: Miranda v. Arizona, 384 U.S. 436\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Under Miranda v. Arizona, 384 U.S. 436 (1966), law enforcement must inform suspects of their Miranda rights before custodial interrogation, including the right to remain silent and the right to an attorney.'\n",
        "    },\n",
        "    # Constitutional Law - Qualified Immunity\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nGovernment officials are entitled to qualified immunity unless their conduct violates clearly established statutory or constitutional rights. See Harlow v. Fitzgerald, 457 U.S. 800 (1982). The right must be sufficiently clear that a reasonable official would understand that what he is doing violates that right. See Anderson v. Creighton, 483 U.S. 635 (1987).\\n\\nCitations found: Harlow v. Fitzgerald, 457 U.S. 800, Anderson v. Creighton, 483 U.S. 635\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Qualified immunity protects government officials unless their conduct violates clearly established rights under Harlow v. Fitzgerald, 457 U.S. 800 (1982), and the right must be sufficiently clear under Anderson v. Creighton, 483 U.S. 635 (1987).'\n",
        "    },\n",
        "    # Evidence - Attorney-Client Privilege\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe attorney-client privilege protects confidential communications between a client and attorney made for the purpose of obtaining legal advice. See Upjohn Co. v. United States, 449 U.S. 383 (1981). The privilege belongs to the client and may be waived only by the client. See Swidler & Berlin v. United States, 524 U.S. 399 (1998).\\n\\nCitations found: Upjohn Co. v. United States, 449 U.S. 383, Swidler & Berlin v. United States, 524 U.S. 399\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The attorney-client privilege protects confidential communications for legal advice under Upjohn Co. v. United States, 449 U.S. 383 (1981), and the privilege belongs to the client and may only be waived by the client under Swidler & Berlin v. United States, 524 U.S. 399 (1998).'\n",
        "    },\n",
        "    # Constitutional Law - Standing\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nTo establish Article III standing, a plaintiff must show injury in fact, causation, and redressability. See Lujan v. Defenders of Wildlife, 504 U.S. 555 (1992). The injury must be concrete and particularized, and actual or imminent. See Lujan v. Defenders of Wildlife, 504 U.S. 555 (1992).\\n\\nCitations found: Lujan v. Defenders of Wildlife, 504 U.S. 555\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Article III standing requires injury in fact, causation, and redressability under Lujan v. Defenders of Wildlife, 504 U.S. 555 (1992), and the injury must be concrete, particularized, and actual or imminent.'\n",
        "    },\n",
        "    # Civil Procedure - Discovery Sanctions\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nA court may impose sanctions for failure to comply with discovery orders under Federal Rule of Civil Procedure 37(b). See Fed. R. Civ. P. 37(b). Sanctions may include dismissal of the action or default judgment. See Fed. R. Civ. P. 37(b)(2)(A).\\n\\nCitations found: Fed. R. Civ. P. 37(b), Fed. R. Civ. P. 37(b)(2)(A)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Courts may impose sanctions for discovery violations under Federal Rule of Civil Procedure 37(b), including dismissal or default judgment under Rule 37(b)(2)(A).'\n",
        "    },\n",
        "    # Constitutional Law - Equal Protection\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe Equal Protection Clause requires that similarly situated persons be treated alike. See City of Cleburne v. Cleburne Living Center, 473 U.S. 432 (1985). Strict scrutiny applies to classifications based on race. See Loving v. Virginia, 388 U.S. 1 (1967).\\n\\nCitations found: City of Cleburne v. Cleburne Living Center, 473 U.S. 432, Loving v. Virginia, 388 U.S. 1\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The Equal Protection Clause requires similar treatment of similarly situated persons under City of Cleburne v. Cleburne Living Center, 473 U.S. 432 (1985), and strict scrutiny applies to racial classifications under Loving v. Virginia, 388 U.S. 1 (1967).'\n",
        "    },\n",
        "    # Evidence - Work Product\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe work product doctrine protects materials prepared in anticipation of litigation. See Hickman v. Taylor, 329 U.S. 495 (1947). This protection is codified in Federal Rule of Civil Procedure 26(b)(3). See Fed. R. Civ. P. 26(b)(3).\\n\\nCitations found: Hickman v. Taylor, 329 U.S. 495, Fed. R. Civ. P. 26(b)(3)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The work product doctrine protects litigation materials under Hickman v. Taylor, 329 U.S. 495 (1947), and is codified in Federal Rule of Civil Procedure 26(b)(3).'\n",
        "    },\n",
        "    # Constitutional Law - Due Process\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nProcedural due process requires notice and an opportunity to be heard. See Mullane v. Central Hanover Bank & Trust Co., 339 U.S. 306 (1950). The notice must be reasonably calculated to inform interested parties. See Mullane v. Central Hanover Bank & Trust Co., 339 U.S. 306 (1950).\\n\\nCitations found: Mullane v. Central Hanover Bank & Trust Co., 339 U.S. 306\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Procedural due process requires notice and opportunity to be heard under Mullane v. Central Hanover Bank & Trust Co., 339 U.S. 306 (1950), and notice must be reasonably calculated to inform interested parties.'\n",
        "    },\n",
        "    # Antitrust Law\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nSection 1 of the Sherman Act prohibits contracts, combinations, or conspiracies in restraint of trade. See 15 U.S.C. § 1. Section 2 prohibits monopolization or attempts to monopolize. See 15 U.S.C. § 2.\\n\\nCitations found: 15 U.S.C. § 1, 15 U.S.C. § 2\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Section 1 of the Sherman Act (15 U.S.C. § 1) prohibits contracts in restraint of trade, while Section 2 (15 U.S.C. § 2) prohibits monopolization or attempts to monopolize.'\n",
        "    },\n",
        "    # Employment Law\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nTitle VII of the Civil Rights Act prohibits employment discrimination based on race, color, religion, sex, or national origin. See 42 U.S.C. § 2000e-2. A plaintiff may establish discrimination through direct evidence or the McDonnell Douglas burden-shifting framework. See McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973).\\n\\nCitations found: 42 U.S.C. § 2000e-2, McDonnell Douglas Corp. v. Green, 411 U.S. 792\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Title VII (42 U.S.C. § 2000e-2) prohibits employment discrimination, and plaintiffs may establish discrimination through direct evidence or the McDonnell Douglas framework from McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973).'\n",
        "    },\n",
        "    # Intellectual Property - Patents\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nTo establish patent infringement, the patentee must show that the accused product or process meets each element of at least one claim of the patent. See 35 U.S.C. § 271. The doctrine of equivalents allows finding infringement even when not literally infringing. See Warner-Jenkinson Co. v. Hilton Davis Chemical Co., 520 U.S. 17 (1997).\\n\\nCitations found: 35 U.S.C. § 271, Warner-Jenkinson Co. v. Hilton Davis Chemical Co., 520 U.S. 17\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Patent infringement requires showing the accused product meets each element of a claim under 35 U.S.C. § 271, and the doctrine of equivalents from Warner-Jenkinson Co. v. Hilton Davis Chemical Co., 520 U.S. 17 (1997) allows finding infringement beyond literal infringement.'\n",
        "    },\n",
        "    # Securities Law\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nSection 10(b) of the Securities Exchange Act prohibits fraudulent practices in connection with the purchase or sale of securities. See 15 U.S.C. § 78j(b). Rule 10b-5 implements this prohibition. See 17 C.F.R. § 240.10b-5.\\n\\nCitations found: 15 U.S.C. § 78j(b), 17 C.F.R. § 240.10b-5\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Section 10(b) of the Securities Exchange Act (15 U.S.C. § 78j(b)) prohibits securities fraud, and Rule 10b-5 (17 C.F.R. § 240.10b-5) implements this prohibition.'\n",
        "    },\n",
        "    # Contracts\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nWhen interpreting contracts, courts give effect to the plain meaning of unambiguous terms. See Restatement (Second) of Contracts § 202. Parol evidence is inadmissible to contradict unambiguous written terms. See Restatement (Second) of Contracts § 213.\\n\\nCitations found: Restatement (Second) of Contracts § 202, Restatement (Second) of Contracts § 213\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Courts give effect to plain meaning of unambiguous contract terms under Restatement (Second) of Contracts § 202, and parol evidence is inadmissible to contradict unambiguous terms under § 213.'\n",
        "    },\n",
        "    # Bankruptcy Law\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nA bankruptcy discharge releases the debtor from personal liability for dischargeable debts. See 11 U.S.C. § 524. Certain debts are excepted from discharge, including student loans unless repayment would impose undue hardship. See 11 U.S.C. § 523(a)(8).\\n\\nCitations found: 11 U.S.C. § 524, 11 U.S.C. § 523(a)(8)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Bankruptcy discharge releases debtors from liability under 11 U.S.C. § 524, but certain debts are excepted from discharge, including student loans under 11 U.S.C. § 523(a)(8) unless repayment would cause undue hardship.'\n",
        "    },\n",
        "    # Criminal Procedure - Fourth Amendment\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nA search violates the Fourth Amendment unless it is reasonable. See Katz v. United States, 389 U.S. 347 (1967). Warrantless searches are per se unreasonable unless they fall within a recognized exception. See Coolidge v. New Hampshire, 403 U.S. 443 (1971).\\n\\nCitations found: Katz v. United States, 389 U.S. 347, Coolidge v. New Hampshire, 403 U.S. 443\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Searches must be reasonable under the Fourth Amendment per Katz v. United States, 389 U.S. 347 (1967), and warrantless searches are per se unreasonable unless falling within an exception under Coolidge v. New Hampshire, 403 U.S. 443 (1971).'\n",
        "    },\n",
        "    # ERISA\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nERISA fiduciaries must act solely in the interest of plan participants and beneficiaries. See 29 U.S.C. § 1104(a)(1). This duty of loyalty requires fiduciaries to avoid conflicts of interest. See Donovan v. Bierwirth, 680 F.2d 263 (2d Cir. 1982).\\n\\nCitations found: 29 U.S.C. § 1104(a)(1), Donovan v. Bierwirth, 680 F.2d 263\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'ERISA fiduciaries must act solely in participants\\' interests under 29 U.S.C. § 1104(a)(1), and the duty of loyalty requires avoiding conflicts of interest under Donovan v. Bierwirth, 680 F.2d 263 (2d Cir. 1982).'\n",
        "    },\n",
        "    # Intellectual Property - Trademarks\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nTrademark infringement requires showing a likelihood of confusion. See 15 U.S.C. § 1114. Courts consider factors such as similarity of marks, similarity of goods, and strength of the mark. See Polaroid Corp. v. Polarad Electronics Corp., 287 F.2d 492 (2d Cir. 1961).\\n\\nCitations found: 15 U.S.C. § 1114, Polaroid Corp. v. Polarad Electronics Corp., 287 F.2d 492\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Trademark infringement requires likelihood of confusion under 15 U.S.C. § 1114, and courts consider factors including mark similarity and strength under Polaroid Corp. v. Polarad Electronics Corp., 287 F.2d 492 (2d Cir. 1961).'\n",
        "    },\n",
        "    # Intellectual Property - Copyright\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nFair use is a defense to copyright infringement. See 17 U.S.C. § 107. Courts consider the purpose and character of use, nature of the work, amount used, and effect on the market. See Campbell v. Acuff-Rose Music, Inc., 510 U.S. 569 (1994).\\n\\nCitations found: 17 U.S.C. § 107, Campbell v. Acuff-Rose Music, Inc., 510 U.S. 569\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Fair use is a defense to copyright infringement under 17 U.S.C. § 107, and courts consider purpose, nature, amount, and market effect under Campbell v. Acuff-Rose Music, Inc., 510 U.S. 569 (1994).'\n",
        "    },\n",
        "    # Employment Law - ADA\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nThe Americans with Disabilities Act requires employers to provide reasonable accommodations to qualified individuals with disabilities. See 42 U.S.C. § 12112(b)(5)(A). An accommodation is reasonable if it does not impose an undue hardship. See 42 U.S.C. § 12111(10).\\n\\nCitations found: 42 U.S.C. § 12112(b)(5)(A), 42 U.S.C. § 12111(10)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'The ADA requires reasonable accommodations for qualified individuals with disabilities under 42 U.S.C. § 12112(b)(5)(A), and accommodations must not impose undue hardship under 42 U.S.C. § 12111(10).'\n",
        "    },\n",
        "    # RICO\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nRICO prohibits conducting an enterprise through a pattern of racketeering activity. See 18 U.S.C. § 1962(c). A pattern requires at least two predicate acts within ten years. See 18 U.S.C. § 1961(5).\\n\\nCitations found: 18 U.S.C. § 1962(c), 18 U.S.C. § 1961(5)\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'RICO prohibits conducting an enterprise through racketeering activity under 18 U.S.C. § 1962(c), and a pattern requires at least two predicate acts within ten years under 18 U.S.C. § 1961(5).'\n",
        "    },\n",
        "    # Torts - Product Liability\n",
        "    {\n",
        "        'prompt': 'Analyze the following legal document excerpt and create a summary with accurate citations.\\n\\nDocument Excerpt:\\nStrict product liability requires showing the product was defective and unreasonably dangerous. See Restatement (Second) of Torts § 402A. A product may be defective in design, manufacture, or warning. See Restatement (Third) of Torts: Products Liability § 2.\\n\\nCitations found: Restatement (Second) of Torts § 402A, Restatement (Third) of Torts: Products Liability § 2\\n\\nInstructions:\\n1. Create a concise summary of the key points\\n2. Include all relevant citations in your summary\\n3. Ensure every factual claim is tied to a specific citation\\n4. Do not make up information or citations\\n\\nSummary:',\n",
        "        'completion': 'Strict product liability requires a defective and unreasonably dangerous product under Restatement (Second) of Torts § 402A, and defects may be in design, manufacture, or warning under Restatement (Third) of Torts: Products Liability § 2.'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save fine-tuning data\n",
        "with open('data/train.jsonl', 'w') as f:\n",
        "    for item in finetune_examples:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# Validation data (20% split)\n",
        "val_size = max(1, len(finetune_examples) // 5)\n",
        "with open('data/val.jsonl', 'w') as f:\n",
        "    for item in finetune_examples[:val_size]:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"✓ Created {len(finetune_examples)} fine-tuning examples\")\n",
        "print(f\"✓ Created {val_size} validation examples\")\n",
        "print(f\"✓ Training examples cover: Civil Procedure, Evidence, Constitutional Law, Employment, IP, Securities, Antitrust, Bankruptcy, and more\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRqgx1rzg0hE"
      },
      "source": [
        "## Fine-tune Llama 3\n",
        "\n",
        "**⚠️ Make sure you've uploaded all project source files before running this!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPxYjs5jg0hE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71efc34b-b29d-49e8-b0c9-4162712fedcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created 28 DPO training pairs\n",
            "✓ Created 2 DPO validation pairs\n",
            "\n",
            "✅ All training data generated!\n"
          ]
        }
      ],
      "source": [
        "# Generate DPO training data (chosen vs rejected pairs)\n",
        "dpo_examples = []\n",
        "\n",
        "for ex in finetune_examples:\n",
        "    prompt = ex['prompt']\n",
        "    chosen = ex['completion']  # Good response with citations\n",
        "\n",
        "    # Create rejected variants (without citations or with hallucinations)\n",
        "    rejected_variants = [\n",
        "        # Variant 1: Remove citations\n",
        "        re.sub(r'\\([^)]*\\d+\\s+[A-Z][a-z]+\\s+\\d+[^)]*\\)', '', chosen).strip(),\n",
        "        # Variant 2: Generic response without specific details\n",
        "        'The court made a decision based on the legal principles and facts presented in the case.',\n",
        "        # Variant 3: Add fake citation\n",
        "        chosen.replace('Smith v. Jones', 'Brown v. White, 789 F.2d 123 (2019)') if 'Smith' in chosen else chosen + ' (See Johnson v. Smith, 456 F.3d 789 (2021))'\n",
        "    ]\n",
        "\n",
        "    # Use first variant that's different from chosen\n",
        "    rejected = rejected_variants[0]\n",
        "    if rejected == chosen or len(rejected) < 20:\n",
        "        rejected = rejected_variants[1]\n",
        "\n",
        "    dpo_examples.append({\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen,\n",
        "        'rejected': rejected\n",
        "    })\n",
        "\n",
        "# Save DPO data\n",
        "with open('data/dpo_train.jsonl', 'w') as f:\n",
        "    for item in dpo_examples:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "with open('data/dpo_val.jsonl', 'w') as f:\n",
        "    for item in dpo_examples[:2]:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"✓ Created {len(dpo_examples)} DPO training pairs\")\n",
        "print(f\"✓ Created {len(dpo_examples[:2])} DPO validation pairs\")\n",
        "print(\"\\n✅ All training data generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsBqm5j_g0hF"
      },
      "outputs": [],
      "source": [
        "# Run fine-tuning\n",
        "!python train_finetune.py --config configs/finetune_config.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd8Pr2dDg0hF"
      },
      "source": [
        "## DPO Training\n",
        "\n",
        "Run this after fine-tuning is complete. First, create DPO pairs if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfcuc_Etg0hF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c524f635-2ffd-462d-fb46-fbb55264f45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ DPO data ready\n"
          ]
        }
      ],
      "source": [
        "# DPO data already generated above - ready to train!\n",
        "print(\"✓ DPO data ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtRRTn8mg0hF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c3bfcd-2873-4597-8062-ffe6f9152b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 06:05:37.393221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765260337.412938    2645 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765260337.418825    2645 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765260337.433559    2645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765260337.433584    2645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765260337.433588    2645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765260337.433593    2645 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 06:05:37.438135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_dpo.py\", line 13, in <module>\n",
            "    from src.data_processing.dataset_utils import load_jsonl, create_dpo_dataset\n",
            "  File \"/content/src/data_processing/__init__.py\", line 1, in <module>\n",
            "    from .legal_document_processor import LegalDocumentProcessor\n",
            "ModuleNotFoundError: No module named 'src.data_processing.legal_document_processor'\n"
          ]
        }
      ],
      "source": [
        "# Run DPO training\n",
        "!python train_dpo.py --config configs/dpo_config.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqdjfMN4g0hF"
      },
      "source": [
        "## Save Models to Google Drive\n",
        "\n",
        "**Important**: Colab files are temporary. Always save models to Google Drive!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNooexMNg0hF"
      },
      "outputs": [],
      "source": [
        "# Copy models to Google Drive\n",
        "import shutil\n",
        "\n",
        "DRIVE_MODELS_DIR = '/content/drive/MyDrive/legal_document_models'\n",
        "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Copy fine-tuned model\n",
        "if os.path.exists('models/finetuned_llama3'):\n",
        "    shutil.copytree('models/finetuned_llama3',\n",
        "                    f'{DRIVE_MODELS_DIR}/finetuned_llama3',\n",
        "                    dirs_exist_ok=True)\n",
        "    print(\"✓ Fine-tuned model saved to Drive\")\n",
        "\n",
        "# Copy DPO model\n",
        "if os.path.exists('models/dpo_llama3'):\n",
        "    shutil.copytree('models/dpo_llama3',\n",
        "                    f'{DRIVE_MODELS_DIR}/dpo_llama3',\n",
        "                    dirs_exist_ok=True)\n",
        "    print(\"✓ DPO model saved to Drive\")\n",
        "\n",
        "print(f\"\\nModels saved to: {DRIVE_MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1sl2Rr0g0hF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp_YH5y7g0hF"
      },
      "outputs": [],
      "source": [
        "# Test inference\n",
        "import sys\n",
        "sys.path.append('/content/legal-document-analysis')\n",
        "\n",
        "from src.models.llama3_model import Llama3Model\n",
        "\n",
        "# Load model (use DPO model if available, otherwise fine-tuned)\n",
        "model_path = 'models/dpo_llama3' if os.path.exists('models/dpo_llama3') else 'models/finetuned_llama3'\n",
        "if not os.path.exists(model_path):\n",
        "    model_path = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "    print(\"⚠️ Using base model (trained models not found)\")\n",
        "\n",
        "print(f\"Loading model from: {model_path}\")\n",
        "\n",
        "model = Llama3Model(\n",
        "    model_name=model_path,\n",
        "    use_4bit=True\n",
        ")\n",
        "\n",
        "# Test generation\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful legal document analyst assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Analyze the following legal document excerpt and create a summary with accurate citations.\n",
        "\n",
        "Document Excerpt:\n",
        "The court held that the defendant's motion to dismiss should be granted. See Smith v. Jones, 123 F.3d 456 (2020). The plaintiff failed to state a claim under Rule 12(b)(6).\n",
        "\n",
        "Summary:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nGenerating response...\")\n",
        "response = model.generate(prompt, max_length=256, temperature=0.7)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generated Response:\")\n",
        "print(\"=\"*60)\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}